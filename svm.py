# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D3laFXgRVROiqg8A5gnvX7_VHRbWBwuS

# Support Vector Machine (SVM)
[Explicação SVM](https://www.datacamp.com/tutorial/svm-classification-scikit-learn-python)
Conjunto de métodos de aprendizado supervisionado usados para classificação, regressão e detecção de outliers.

Dado um conjunto de dados de treino, SVM cria um modelo que assimila novos exemplos em uma categoria, utilizando um classificador não probabilístico. São mapeados de forma que as categorias são divididas com a maior distância possível entre eles (hiperplano linear: novo plano que os dados pertencem que maximiza a margem entre as classes; os pontos que tocam as linhas das margens são conhecidos como suport vectors).

Para dados não lineares:não há um unico plano para a separação deles. kernel trick -> aplica uma transformação linear (espaço dimensional superior) e aplica-se o hiperplano, em seguida, volta para a aplicação inversa dos dados e volta para o plano de origem.

*   **Vantagens**:
Eficaz nos casos em que o número de dimensões é maior que o número de amostras.

Usa um subconjunto de pontos de treinamento na função de decisão (chamados de vetores de suporte), portanto, também é eficiente em termos de memória.


*   **Desvantagens**:

Se o número de recursos for muito maior que o número de amostras, evite o ajuste excessivo na escolha das funções do Kernel e o termo de regularização é crucial.

 Não fornecem estimativas de probabilidade diretamente, elas são calculadas usando uma validação five-fold cross-validation
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#plot sem usar o plt.show():
# %matplotlib inline      
!pip install scikit-learn

#Dados Iris para Câncer de mama
#Criar um modelo para classificar os tumores em malignos e benignos
 
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()

print(cancer.keys())

print(cancer['DESCR'])

#Configuração do DF 

df_cancer = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])
print(df_cancer.info()) #dados totalmente numérico

#Df de target, precisamos passar as respostas (y) para o modelo

df_targets = pd.DataFrame( cancer['target'],columns=['Cancer'])

print(df_targets)

#Construção do Modelo

#Biblioteca responsável por separar os dados em teste e treino e associar os parâmetros 
from sklearn.model_selection import train_test_split

"""1.   Faz a divisão dos dados (x e y) em duas partes para manter a 
 relações entre os dados.
 2.   A partir do item anterior,cria-se o modelo :

 *   **Conjunto de treino:** aprende a relação entre os parâmetros com a variável pevista 
 *   **Conjunto de teste:** faz o teste dos conjuntos de dados do item anterior 


"""

X = df_cancer

y = np.ravel(df_targets) #transforma em array
#y = df_targets

#Separação da quantidade de dados destinadas a teste e treino (30% como teste e 70% treino)
#A resposta é uma túpula com 4 elementos: podemos usar o desempacotamento de tupulas(,random_state=101) 

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30)

X_train

#Import SVM
from sklearn import svm


model = svm.SVC()

model.fit(X_train,y_train)

print(X_train.shape)
print('\n')
print(y_test.shape)

pred= model.predict(X_test)

from sklearn.metrics import classification_report,confusion_matrix #modelo de classificação de ML, output= target

print(classification_report(y_test,pred))
print('*'*100)
print(confusion_matrix(y_test,pred))

# O modelo está com 98% de precisão

##Grid search 

from sklearn.model_selection import GridSearchCV,KFold
svc = svm.SVC()

"""# **Grid Search :**⚡
Testa as possíveis combinações de parâmetros para melhorar o modelo.  Implementa “score_samples”, “predict”, “predict_proba”, “decision_function”, “transform” e “inverse_transform” 

Os parâmetros do estimador usados ​​para aplicar esses métodos são otimizados por busca de grade validada cruzada sobre uma grade de parâmetros.


[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)


---


*   Alguns Parâmetros modificados aqui:

---



1.   **C:** controla o custo de classificações erradas
2.  Gamma: altera o comportamento da função gauss (tipo de kernel que está sendo utilizado)

3.  **Kernel:** transforma o dataset de input em  outro formato. Ex.: Radial Basis Function (RBF)









"""

print(X_train.shape)
print('\n')
print(y_train.shape)

#---------------------------------------------------
p_grid={'C':[0.1,1,10,100,1000],
            'gamma':[1,0.1,0.01,0.001,0.0001],
            'kernel':['rbf']}
#---------------------------------------------------

#verbose=altera os outputs

grid= GridSearchCV(svc,p_grid,refit=True,verbose=3)

grid.fit(X_train,y_train)

pred = grid.predict(X_test)

print(classification_report(y_test,pred))
print('*'*100)
print(confusion_matrix(y_test,pred))

# O modelo está com 95% de precisão